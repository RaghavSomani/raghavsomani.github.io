---
title: "Scaling laws of optimization algorithms for Deep Learning - the Graphon perspective"
collection: projects
type: "Research project"
permalink: /projects/2021-05-01-project-5
excerpt: 'Understanding scaling limits of discrete Euclidean optimization algorithms on large unlabeled graphs. This problem is motivated by the problem of optimizing permutation invariant risk functions of (single layer and deep) Neural Networks (NNs). Theoretical aspects stem from the original theory of gradient flows on the Wasserstein space, which have been used to understand scaling limits of (stochstic) gradient descent ((S)GD) processes in the case of single hidden layer neural networks. There are also other related questions that are specific to the qualitative nature of the stochasticity in the SGD process, and the role of depth in NNs.'
venue: "University of Washington"
date: 2021-05-01
location: "Seattle, WA, USA"
---

Machine Learning models, such as (Deep) Neural Networks ((D)NNs) are growing in size as well as in their efficacy. Gradient-based methods are usually used to train such DNNs. Being able to understand the intricacies of the training dynamics is therefore important to understand its workings. Recent work helps illuminate the non-linear training dynamics of two-layer NNs, with the help of the theory of Wasserstein gradient flows and mean field theory [1,2]. This viewpoint relies on the observation that the permutation symmetry of the hidden layer allows the problem to be viewed as an optimization problem over probability measures.

![A DNN as a computational graph](https://raghavsomani.github.io/projects/images/DNN.png)

DNNs can be considered as large computational graphs and therefore can possess different groups of symmetries. The output of a DNN stays the same if neurons in any of its layers are permuted appropriately. This restricts the possibility of extension of a similar approach and calls for an understanding of the training dynamics over functions that possess such symmetries. My recent work shows that optimization algorithms like (Stochastic) Gradient Descent ((S)GD), have a well-defined analytical scaling limit as the size of the network grows. The existing theory of [graphons](https://en.wikipedia.org/wiki/Graphon){:target="_blank"} (analytical limits of dense unlabeled weighted graphs), and the general theory of gradient flows on metric spaces [3], allows us to characterize this scaling limit. Such a characterization tells us that various properties of the optimization procedure like properties of the minimizers, convexity, convergence speed, etc., also tend to have a limiting behavior.

This brings up some broad questions:
1. How do layers interact? Can we understand the large depth limit?
2. Does the stochasticity in SGD play a crucial role?
3. When do models exhibit fast convergence rates?
4. What can be said about GD over sparse NNs?

The framework of casting such an optimization problem on graphs as gradient flows on graphons allows us to cast certain extremal graph theoretic questions.

![Approximately recovering Mantel's theorem using gradient flows on Graphons](https://raghavsomani.github.io/projects/files/mantel.gif)

In the above animation, it is shown that gradient flows can be used to recover Mantel's theorem, which states that a triangle-free graph with the maximum number of edges is the balanced complete bipartite graph. Here we minimize the difference between ten times the triangle density and the edge density.

An initial work has been accepted at the [OTML workshop](https://otml2021.github.io/){:target="_blank"} at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"}. An [extended version](https://arxiv.org/abs/2111.09459){:target="_blank"} is under review.

Resources on the lines of this research:
1. [OTML poster](https://raghavsomani.github.io/publications/files/OTML_poster_Gradient_Flows_on_Graphons.pdf){:target="_blank"}.
2. [ArXiv](https://arxiv.org/abs/2111.09459){:target="_blank"}.
3. The Kantorovich Initiative retreat 2022 [presentation slides](https://raghavsomani.github.io/projects/files/Gradient_flows_on_Graphons_presentation.pdf){:target="_blank"}.
4. Scaling Laws for Deep Learning Micro-workshop [presentation slides](https://raghavsomani.github.io/projects/files/Scaling_limit_of_optimization_algorithms_on_NNs.pdf){:target="_blank"}.

### References

1. [On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport](https://arxiv.org/abs/1805.09545){:target="_blank"} - Lénaïc Chizat, Francis Bach, 2018.
2. [A Mean Field View of the Landscape of Two-Layer Neural Networks](https://arxiv.org/abs/1804.06561){:target="_blank"} - Song Mei, Andrea Montanari, Phan-Minh Nguyen, 2018.
3. [Gradient Flows: In Metric Spaces and in the Space of Probability Measures](https://www.google.com/books/edition/_/rCDK9JA5BAEC?hl=en&sa=X&ved=2ahUKEwiq-NvV6-L5AhWlGDQIHSRYBFUQre8FegQIAxAX){:target="_blank"} - Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré, 2008.