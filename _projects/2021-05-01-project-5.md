---
title: "Scaling laws of optimization algorithms for Deep Learning - the Graphon perspective"
collection: projects
type: "Research project"
permalink: /projects/2021-05-01-project-5
excerpt: 'Stochastic optimization algorithms are used to train large (Deep) Neural Networks ((D)NNs). The non-linear training dynamics of two-layer NNs can be described as a mean-field interacting particle system where the "particles" are the neurons in the single hidden layer. Wasserstein gradient flows often arise from such mean-field interactions among exchangeable particles. This relies on the observation that the permutation symmetry of the neurons in the hidden layer allows the problem to be viewed as an optimization problem over probability measures. Going beyond, multi-layer NNs can be considered as large computational graphs and therefore can possess different groups of symmetries. This body of work aims to describe analytical scaling limits of stochastic optimization algorithms as the size of the network grows. We use and develop on the existing theory of exchangeable arrays, graphons (analytical limits of dense graphs), the general theory of gradient flows on metric spaces, and insights from propagation of chaos to characterize this scaling limit. We discover a generalized notion of the McKean-Vlasov equation on graphons where the phenomenon of _propagation of chaos_ holds. In the asymptotically zero-noise case, this limit is a gradient flow on the metric space of graphons.'
venue: "University of Washington"
date: 2021-05-01
location: "Seattle, WA, USA"
---

Machine Learning models, such as (Deep) Neural Networks ((D)NNs), are growing in size as well as in their efficacy. Gradient-based stochastic optimization algorithms are commonly used to train deep learning models. It is thus important to understand the intricacies of the training dynamics of such large networks. In a simplified model like a two-layer Neural Network (NN), the training dynamics can be considered as a system where each hidden neuron is a "particle" interacting with the others. This concept is often connected to Wasserstein gradient flows, which emerge from these mean-field interactions among exchangeable particles [1,2]. This relies on the observation that the permutation symmetry of the neurons in the hidden layer allows the problem to be viewed as an optimization problem over probability measures.

![A DNN as a computational graph](https://raghavsomani.github.io/projects/images/DNN.png)

In more complex models, such as multi-layer NNs, that can be considered as large structured computational graphs, these "particles" are the weights of the connections between neurons. The individual neurons in any given layer can be rearranged without changing the output of the network. In such an application, the interacting "particles" are edge weights in a graph whose vertex labels are exchangeable but not the edges themselves. This is a different symmetry at play and restricts the possibility of extension of similar approaches as used earlier [1,2]. Motivated by such optimization problems on graphs, we investigate the question for functions over this class of symmetries.

My recent work shows that **stochastic optimization algorithms have a well-defined analytical scaling limit as the size of the network grows**. The existing theory of [graphons](https://en.wikipedia.org/wiki/Graphon){:target="_blank"} (analytical limits of dense unlabeled weighted graphs), and the general theory of gradient flows on metric spaces [3], allow us to characterize a scaling limit of stochastic optimization algorithms as smooth deterministic curves on the space of graphons. We discover a generalized notion of the McKean-Vlasov equations on graphons where the phenomenon of _propagation of chaos_ holds. In the asymptotically zero-noise case, this limit is a gradient flow on the metric space of graphons.

Such scaling laws can simplify parameter setting for large experiments and model transfer to new domains. On the theoretical side, scaling laws shed light on empirical phenomena and unify them with mathematical concision.

Research Papers (under review):
1. Gradient flows on graphons: existence, convergence, continuity equations - [ArXiv](https://arxiv.org/abs/2111.09459){:target="_blank"}.
2. Stochastic optimization on matrices and a graphon McKean-Vlasov limit - [ArXiv](https://arxiv.org/abs/2210.00422){:target="_blank"}.

The framework of casting such an optimization problem on graphs as gradient flows on graphons allows us to cast several extremal graph theoretic questions. In the below animation, it is shown that gradient flows can be used to recover Mantel's theorem. It states that a triangle-free graph with the maximum number of edges is the balanced complete bipartite graph. Here we minimize the difference between the triangle density and the edge density simply by using Euclidean Gradient Descent. The symmetric square adjacency matrix is normalized over the unit square, and we see that the dynamics converges to that of a balanced bipartite graph.

![Approximately recovering Mantel's theorem using gradient flows on Graphons](https://raghavsomani.github.io/projects/files/mantel.gif)

An initial work has been accepted at the [OTML workshop](https://otml2021.github.io/){:target="_blank"} at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"}.

Our Presentations:
1. [OTML](https://otml2021.github.io/){:target="_blank"} workshop at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"} - [poster](https://raghavsomani.github.io/publications/files/OTML_poster_Gradient_Flows_on_Graphons.pdf){:target="_blank"}.
2. [The Kantorovich Initiative retreat 2022](https://kantorovich.org/event/ki-retreat-2022/){:target="_blank"} - [presentation slides](https://raghavsomani.github.io/projects/files/Gradient_flows_on_Graphons_presentation.pdf){:target="_blank"}.
3. Scaling Laws for Deep Learning Micro-workshop, UW CSE - [presentation slides](https://drive.google.com/file/d/1w934--CP-0zCwxiCcZATUdCr-vCGfJC4/view?usp=sharing){:target="_blank"}.
4. [Machine Learning Foundations](https://www.microsoft.com/en-us/research/group/mlog/){:target="_blank"} seminar, [Microsoft Research Redmond](https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1W-_T-6YNfjVsbEerDy7-gnJP7ZvanMyi/view?usp=sharing){:target="_blank"}.
5. Talk at [Applied Optimal Transport workshop](https://www.imsi.institute/activities/applied-optimal-transport/){:target="_blank"} at [IMSI](https://www.imsi.institute/){:target="_blank"} - [presentation video](https://www.imsi.institute/videos/gradient-flows-on-graphons/){:target="_blank"}, [presentation slides](https://kantorovich.org/event/ki-retreat-2022/Somani-Tripathi.pdf){:target="_blank"}.
6. Talk at [UBC PDE + Probability seminar](https://secure.math.ubc.ca/Links/ProbSeminar/){:target="_blank"}, [Pacific Institute for the Mathematical Sciences](https://www.pims.math.ca/){:target="_blank"} - [Talk](https://www.pims.math.ca/scientific-event/220915-uppssp){:target="_blank"}.
7. [IFML workshop](https://simons.berkeley.edu/workshops/schedule/22652){:target="_blank"} at [Simons Institute](https://simons.berkeley.edu/){:target="_blank"} - [poster](https://raghavsomani.github.io/projects/files/Scaling_limit_of_optimization_algorithms_on_NNs_IFML.pdf){:target="_blank"}.
8. [IFDS](https://ifds.info/){:target="_blank"} seminar - [presentation slides](https://drive.google.com/file/d/1qKmv3Kv-nrzB7_lKQKBvTyGVRN6FyFZF/view?usp=share_link){:target="_blank"}.
9. [Poster presentation](https://sites.google.com/view/lpsxv/posters?authuser=0#h.4q83to7wlekt){:target="_blank"} at [Lectures in Probability and Stochastic Processes XV](https://sites.google.com/view/lpsxv/home?authuser=0){:target="_blank"}.
10. [Google India Research Lab](https://research.google/teams/india-research-lab/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1k9QwbRYc1APCLqZmZDd1VrE3lmkwOEGI/view?usp=sharing){:target="_blank"}.
11. [Talk](https://sites.google.com/view/syscontalks/home?authuser=0){:target="_blank"} at the [System and Control Engineering department](https://www.sc.iitb.ac.in/){:target="_blank"} at the [Indian Institute of Technology Bombay](https://www.iitb.ac.in/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1k9QwbRYc1APCLqZmZDd1VrE3lmkwOEGI/view?usp=sharing){:target="_blank"}.
12. [Talk](https://www.tcs.tifr.res.in/events/scaling-limits-stochastic-optimization-algorithms-over-large-graphs){:target="_blank"} at the [School of Technology and Computer Science](https://www.tcs.tifr.res.in/){:target="_blank"} at the [Tata Institute of Fundamental Research](https://www.tifr.res.in/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1k9QwbRYc1APCLqZmZDd1VrE3lmkwOEGI/view?usp=sharing){:target="_blank"}, [YouTube video](https://www.youtube.com/watch?v=cCC2zOf2Jqc){:target="_blank"}.
13. [Lecture](https://www.cse.iitb.ac.in/research/talks.php?year=2023&id=1623){:target="_blank"} on basics of Graphons at the [Department of Computer Science and Engineering](https://www.cse.iitb.ac.in/){:target="_blank"} at the [Indian Institute of Technology Bombay](https://www.iitb.ac.in/){:target="_blank"}.
14. [Talk](https://www.cse.iitb.ac.in/research/talks.php?year=2023&id=1622){:target="_blank"} at the [Department of Computer Science and Engineering](https://www.cse.iitb.ac.in/){:target="_blank"} at the [Indian Institute of Technology Bombay](https://www.iitb.ac.in/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1k9QwbRYc1APCLqZmZDd1VrE3lmkwOEGI/view?usp=sharing){:target="_blank"}.
15. [Junior talk](https://kantorovich.org/event/ki-retreat-2023/#scaling-limit-of-sgd-over-large-networks){:target="_blank"} at the [IFML + KI retreat 2023](https://kantorovich.org/event/ki-retreat-2023/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1kB7iSm7ltO5Bj9xdmagBDUKEhbaBthbn/view?usp=share_link){:target="_blank"}.
16. Short talk at [IFML workshop 2023](https://ifml-uw.github.io/IFML-Workshop-2023/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1pBHQb-ekKdaKE8S2i9REXbiBPokV-EbJ/view?usp=share_link){:target="_blank"}.
17. Probability seminar [talk](https://events.brown.edu/event/257532-probability-seminar-presents-soumik-pal-university){:target="_blank"} at Brown University.
18. Student prsentation at the [Kantorovich Initiative + Scale MoDL retreat 2023](https://kantorovich.org/event/ki-retreat-2023-2/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1D9zwLJyihLIgvrK10dnUfUdkbfHI0pKE/view?usp=share_link){:target="_blank"}.
19. Generals talk at UW CSE - [presentation slides](https://drive.google.com/file/d/104XM6tZmJS-RDvye4a4AydUF6acKZ32J/view?usp=drive_link){:target="_blank"}.

As ongoing work,

1. We are examining different kinds of graph dynamics to find their appropriate scaling limits. For instance, sampling algorithms like the Metropolis-Hastings algorithm also have such a scaling limit, showing a fascinating link between sampling and optimization.
2. We want to have a more robust understanding of limits of exchangeable arrays and **attempt to generalize the Wasserstein calculus to higher-order exchangeable structures**.
3. We further want to build a theory that allows us to simultaneously understand the scaling limits of infinitely wide and infinitely deep neural networks posed as an optimal stochastic control problem. This can throw light on the ways in which layers interact with each other during the optimization process.

### References

1. [On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport](https://arxiv.org/abs/1805.09545){:target="_blank"} - Lénaïc Chizat, Francis Bach, 2018.
2. [A Mean Field View of the Landscape of Two-Layer Neural Networks](https://arxiv.org/abs/1804.06561){:target="_blank"} - Song Mei, Andrea Montanari, Phan-Minh Nguyen, 2018.
3. [Gradient Flows: In Metric Spaces and in the Space of Probability Measures](https://www.google.com/books/edition/_/rCDK9JA5BAEC?hl=en&sa=X&ved=2ahUKEwiq-NvV6-L5AhWlGDQIHSRYBFUQre8FegQIAxAX){:target="_blank"} - Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré, 2008.
4. [Large networks and graph limits](https://www.google.com/books/edition/Large_Networks_and_Graph_Limits/FsFqHLid8sAC?hl=en){:target="_blank"} - László Lovász