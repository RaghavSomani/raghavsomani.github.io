---
title: "Scaling Limits of Algorithms on Large Matrices"
collection: projects
type: "Ph.D. thesis"
permalink: /projects/2024-08-16-project-6
# excerpt: ''
# excerpt: 'Advances in large-scale machine learning have been driven by simple and lightweight iterative algorithms. In my Ph.D. thesis, I explore a broad class of such algorithms that operate on large random matrices, where matrix coordinate processes interact through mean-field dynamics. Examples include stochastic gradient-based methods for optimizing deep neural networks (DNNs), Markov chain algorithms for sampling from random matrix models, and the forward pass algorithm in DNNs. These iterative algorithms and dynamics on large finite-dimensional matrices exhibit well-defined analytical scaling limits as algorithm step sizes approach zero and matrix dimensionality grows to infinity. These scaling limits can be described as processes on infinite exchangeable arrays (IEAs) and analytically characterized as smooth curves on the metric space of graphons and measure-valued graphons (MVGs). These curves can also be described via McKean-Vlasov type stochastic differential equations (SDEs), similar to those in the theory of interacting particle systems. The analysis reveals that the scaling limits of popular algorithms like stochastic gradient descent (SGD) and Metropolis chain Monte Carlo sampling coincide and are gradient flows on the space of graphons, highlighting a surprising connection between sampling and optimization. This also demonstrates the propagation of chaos phenomenon in large-scale systems, indicating that as system size grows, the coordinate evolutions become statistically independent. We apply these tools to analyze feedforward dynamics in a linear residual network as its depth and width increase. As the network size grows, the evolution of any finite set of neurons, from the input layer to the output layer, for any fixed input, becomes independent (propagation of chaos). Moreover, this neuron evolution can be described as a Gaussian process, with drift and diffusion components determined by the network's weights, providing an optimal control approach for the risk minimization problem in infinitely deep and wide networks.'
# excerpt: 'This work has been accepted as my Ph.D. thesis at the [University of Washington](https://www.washington.edu/). Please find the full text of the thesis [here](https://raghavsomani.github.io/projects/files/thesis.pdf).'
venue: "University of Washington"
date: 2024-08-16
location: "Seattle, WA, USA"
---

This work has been accepted as my Ph.D. thesis at the [University of Washington](https://www.washington.edu/){:target="_blank"}. Please find the full text of the thesis [here](https://raghavsomani.github.io/projects/files/thesis.pdf){:target="_blank"}.

