---
title: "Gradient flows on Graphons"
collection: projects
type: "Research project"
permalink: /projects/2021-05-01-project-5
excerpt: 'Understanding scaling limits of gradient flow processes on large unlabeled graphs. This problem is motivated by the problem of optimizing permutation invariant risk functions of (single layer and deep) Neural Networks. Theoretical aspects stem from the original theory of gradient flows on the Wasserstein space, which have been used to understand scaling limits of (stochstic) gradient descent ((S)GD) processes in the case of single hidden layer neural networks. There are also other related questions that are specific to the qualitative nature of the stochasticity (sub-gaissian vs heavy tailed) in the SGD process.'
venue: "University of Washington"
date: 2021-05-01
location: "Seattle, WA, USA"
---

Wasserstein gradient flows on probability measures have found a host of applications in various optimization problems. They typically arise as the continuum limit of exchangeable particle systems evolving by some mean-field interaction involving a gradient-type potential. However, in many problems, such as in multi-layer neural networks, the so-called particles are edge weights on large graphs whose nodes are exchangeable. Such large graphs are known to converge to continuum limits called graphons as their size grow to infinity. We show that the Euclidean gradient flow of a suitable function of the edge-weights converges to a novel continuum limit given by a curve on the space of graphons that can be appropriately described as a gradient flow or, more technically, a curve of maximal slope. Several natural functions on graphons, such as homomorphism functions and the scalar entropy, are covered by our set-up, and the examples have been worked out in detail.

![Approximately recovering Mantel's theorem using gradient flows on Graphons](https://raghavsomani.github.io/projects/files/mantel.gif)

In the above animation, it is shown that a simple gradient flow can be used to recover Mantel's theorem, which states that a triangle-free graph with the maximum number of edges is the balanced complete bipartite graph.

The paper has been accepted at the [OTML workshop](https://otml2021.github.io/){:target="_blank"} at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"}.

Resources on the lines of this research:
1. [OTML poster](https://raghavsomani.github.io/publications/files/OTML_poster_Gradient_Flows_on_Graphons.pdf){:target="_blank"}.
2. [ArXiv](https://arxiv.org/abs/2111.09459){:target="_blank"}.
3. [Presentation slides](https://raghavsomani.github.io/projects/files/Gradient_flows_on_Graphons_presentation.pdf){:target="_blank"}