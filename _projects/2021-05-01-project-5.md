---
title: "Scaling laws of optimization algorithms for Deep Learning - the Graphon perspective"
collection: projects
type: "Research project"
permalink: /projects/2021-05-01-project-5
excerpt: 'Gradient-based stochastic optimization algorithms are usually used to train large (Deep) Neural Networks ((D)NNs). Recent works have helped illuminate the non-linear training dynamics of two-layer NNs, using the theory of Wasserstein gradient flows and mean field scaling limits. This viewpoint relies on the permutation symmetry of the hidden layer that allows the problem to be viewed as an optimization problem over probability measures. Going beyond the two-layer setting, DNNs can be considered as large computational graphs and therefore can possess different groups of symmetries. This body of work shows that stochastic optimization algorithms over finite networks have a well-defined analytical scaling limit as the size of the network grows. The existing theory of graphons (analytical limits of dense unlabeled weighted graphs), the general theory of gradient flows on metric spaces, and insights from propagation of chaos allow us to characterize this scaling limit. The limiting curve of graphons is characterized by a family of stochastic differential equations and can be thought of as an extension of the classical McKean-Vlasov limit for interacting diffusions to the graphon setting.'
venue: "University of Washington"
date: 2021-05-01
location: "Seattle, WA, USA"
---

Machine Learning models, such as (Deep) Neural Networks ((D)NNs) are growing in size as well as in their efficacy. Gradient-based stochastic optimization based methods are usually used to train such DNNs. Being able to understand the intricacies of the training dynamics is therefore important to understand the workings of large networks. Recent works have helped illuminate the non-linear training dynamics of two-layer NNs, using the theory of _Wasserstein gradient flows_ and _mean field scaling limits_ [1,2]. This viewpoint relies on the observation that the permutation symmetry of the hidden layer allows the problem to be viewed as an optimization problem over probability measures.

![A DNN as a computational graph](https://raghavsomani.github.io/projects/images/DNN.png)

DNNs can be considered as large computational graphs and therefore can possess different groups of symmetries. The output of a DNN stays the same if neurons in any of its layers are permuted appropriately. This restricts the possibility of extension of a similar approaches as used earlier [1,2], and calls for an understanding of the training dynamics over functions that possess such larger group symmetries. My recent work shows that **stochastic optimization algorithms have a well-defined analytical scaling limit as the size of the network grows**. The existing theory of [graphons](https://en.wikipedia.org/wiki/Graphon){:target="_blank"} (analytical limits of dense unlabeled weighted graphs), and the general theory of gradient flows on metric spaces [3], allow us to characterize this scaling limit as a smooth deterministic curve on the space of graphons. We discover a generalized notion of the McKean-Vlasov equations on graphons where the phenomenon of _propagation of chaos_ holds. This is an **attempt to generalize the Wasserstein calculus to higher-order exchangeable structures**. Such a characterization can tell us that various properties of the optimization procedure like properties of the minimizers, 'convexity', convergence rate, etc., should also have an appropriate limiting behavior.

Research Papers:
1. Gradient flows on graphons: existence, convergence, continuity equations - [ArXiv](https://arxiv.org/abs/2111.09459){:target="_blank"}.
2. Stochastic optimization on matrices and a graphon McKean-Vlasov limit - [ArXiv](https://arxiv.org/abs/2210.00422){:target="_blank"}.

The framework of casting such an optimization problem on graphs as gradient flows on graphons allows us to cast several extremal graph theoretic questions. In the below animation, it is shown that gradient flows can be used to recover Mantel's theorem. It states that a triangle-free graph with the maximum number of edges is the balanced complete bipartite graph. Here we minimize the difference between the triangle density and the edge density simply by using Euclidean Gradient Descent.

![Approximately recovering Mantel's theorem using gradient flows on Graphons](https://raghavsomani.github.io/projects/files/mantel.gif)

An initial work has been accepted at the [OTML workshop](https://otml2021.github.io/){:target="_blank"} at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"}.

Presentations:
1. [OTML](https://otml2021.github.io/){:target="_blank"} workshop at [NeurIPS 2021](https://nips.cc/Conferences/2021){:target="_blank"} - [poster](https://raghavsomani.github.io/publications/files/OTML_poster_Gradient_Flows_on_Graphons.pdf){:target="_blank"}.
2. [The Kantorovich Initiative retreat 2022](https://kantorovich.org/event/ki-retreat-2022/){:target="_blank"} - [presentation slides](https://raghavsomani.github.io/projects/files/Gradient_flows_on_Graphons_presentation.pdf){:target="_blank"}.
3. Scaling Laws for Deep Learning Micro-workshop, UW CSE - [presentation slides](https://drive.google.com/file/d/1w934--CP-0zCwxiCcZATUdCr-vCGfJC4/view?usp=sharing){:target="_blank"}.
4. [Machine Learning Foundations](https://www.microsoft.com/en-us/research/group/mlog/){:target="_blank"} seminar, [Microsoft Research Redmond](https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/){:target="_blank"} - [presentation slides](https://drive.google.com/file/d/1W-_T-6YNfjVsbEerDy7-gnJP7ZvanMyi/view?usp=sharing){:target="_blank"}.
5. Talk at [Applied Optimal Transport workshop](https://www.imsi.institute/activities/applied-optimal-transport/){:target="_blank"} at [IMSI](https://www.imsi.institute/){:target="_blank"} - [presentation video](https://www.imsi.institute/videos/gradient-flows-on-graphons/){:target="_blank"}, [presentation slides](https://kantorovich.org/event/ki-retreat-2022/Somani-Tripathi.pdf){:target="_blank"}.
6. Talk at [UBC PDE + Probability seminar](https://secure.math.ubc.ca/Links/ProbSeminar/){:target="_blank"}, [Pacific Institute for the Mathematical Sciences](https://www.pims.math.ca/){:target="_blank"} - [Talk](https://www.pims.math.ca/scientific-event/220915-uppssp){:target="_blank"}.
7. [IFML workshop](https://simons.berkeley.edu/workshops/schedule/22652){:target="_blank"} at [Simons Institute](https://simons.berkeley.edu/){:target="_blank"} - [poster](https://raghavsomani.github.io/projects/files/Scaling_limit_of_optimization_algorithms_on_NNs_IFML.pdf){:target="_blank"}.
8. [IFDS](https://ifds.info/){:target="_blank"} seminar - [presentation slides](https://drive.google.com/file/d/1qKmv3Kv-nrzB7_lKQKBvTyGVRN6FyFZF/view?usp=share_link){:target="_blank"}

There are some broad questions that are open to us:
1. How do layers interact? Can we understand the large depth limit?
2. When do models exhibit fast convergence rates?
3. What can be said about optimization over sparse NNs?
4. Does there exist a stronger but natural topology over graphons that accounts for finer distributional characterstics of edges.

### References

1. [On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport](https://arxiv.org/abs/1805.09545){:target="_blank"} - Lénaïc Chizat, Francis Bach, 2018.
2. [A Mean Field View of the Landscape of Two-Layer Neural Networks](https://arxiv.org/abs/1804.06561){:target="_blank"} - Song Mei, Andrea Montanari, Phan-Minh Nguyen, 2018.
3. [Gradient Flows: In Metric Spaces and in the Space of Probability Measures](https://www.google.com/books/edition/_/rCDK9JA5BAEC?hl=en&sa=X&ved=2ahUKEwiq-NvV6-L5AhWlGDQIHSRYBFUQre8FegQIAxAX){:target="_blank"} - Luigi Ambrosio, Nicola Gigli, Giuseppe Savaré, 2008.
4. [Large networks and graph limits](https://web.cs.elte.hu/~%20lovasz/bookxx/hombook-almost.final.pdf){:target="_blank"} - László Lovász